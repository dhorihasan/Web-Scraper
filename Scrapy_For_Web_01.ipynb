{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Scrapy For Web 01.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Scrap Data From Web Artikel Turnbackhoax and Sabehoaks"
      ],
      "metadata": {
        "id": "_gOe9nq8K6Gm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# install scrapy\n",
        "!pip install Scrapy"
      ],
      "metadata": {
        "id": "TJJD8gBQ3HiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create files for learning\n",
        "!scrapy startproject webscrapy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YSDb21CVF8Pt",
        "outputId": "b4b42cfb-4579-4242-a176-fc75babaff65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New Scrapy project 'webscrapy', using template directory '/usr/local/lib/python3.7/dist-packages/scrapy/templates/project', created in:\n",
            "    /content/webscrapy\n",
            "\n",
            "You can start your first spider with:\n",
            "    cd webscrapy\n",
            "    scrapy genspider example example.com\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get the current working directory\n",
        "import os\n",
        "os.getcwd()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "vf25wYKSVe0u",
        "outputId": "cfbae49f-2b0a-4266-fd37-64ca795bd05a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# change working directories\n",
        "os.chdir('/content/webscrapy/webscrapy/spiders')\n",
        "os.getcwd()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "SupclqVRVxAi",
        "outputId": "f70159a8-753d-4f6d-a61d-2fa0ac091308"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/webscrapy/webscrapy/spiders'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create turnbackhoax.py and save it under the webscrapy/webscrapy/spiders directory\n",
        "%%writefile -a turnbackhoax.py\n",
        "\n",
        "import scrapy\n",
        "\n",
        "\n",
        "class TurnbackhoaxSpider(scrapy.Spider):\n",
        "    name = 'turnbackhoax'\n",
        "    allowed_domains = ['turnbackhoax.id']\n",
        "    #start_urls = ['https://turnbackhoax.id/']\n",
        "\n",
        "    def start_requests(self):\n",
        "        # Editing the default headers (user-agent)\n",
        "        yield scrapy.Request(url='https://turnbackhoax.id/', callback=self.parse,\n",
        "                       headers={'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36'})\n",
        "\n",
        "\n",
        "    def parse(self, response):\n",
        "        article_container = response.xpath('//div[@id=\"main-content\"]/article/div/header')\n",
        "        \n",
        "\n",
        "\n",
        "        for article in article_container:\n",
        "            article_title=article.xpath('.//h3/a/text()').get()\n",
        "            article_date=article.xpath('.//div/span/text()').get()\n",
        "            article_author=article.xpath('.//div/span[2]/a/text()').get()\n",
        "\n",
        "            yield{\n",
        "                'title':article_title,\n",
        "                'date':article_date,\n",
        "                'author':article_author\n",
        "            }\n",
        "\n",
        "        \n",
        "        next_page_url = response.xpath('//div[contains(@class,\"nav-links\")]/a[contains(@class,\"next page-numbers\")]/@href').get()\n",
        "\n",
        "        \n",
        "        \n",
        "        if next_page_url:\n",
        "                yield response.follow(url=next_page_url, callback=self.parse,\n",
        "                headers={'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36'})"
      ],
      "metadata": {
        "id": "MlYYVWTc_egv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1176c34-d27b-4659-f281-c8cde8e5d1b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing turnbackhoax.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create turnbackhoax.py and save it under the webscrapy/webscrapy/spiders directory\n",
        "%%writefile -a saberhoaks.py\n",
        "\n",
        "import scrapy\n",
        "\n",
        "\n",
        "class SaberhoaksSpider(scrapy.Spider):\n",
        "    name = 'saberhoaks'\n",
        "    allowed_domains = ['saberhoaks.jabarprov.go.id']\n",
        "    #start_urls = ['https://saberhoaks.jabarprov.go.id/v2/klarifikasi']\n",
        "\n",
        "    def start_requests(self):\n",
        "        # Editing the default headers (user-agent)\n",
        "        yield scrapy.Request(url='https://saberhoaks.jabarprov.go.id/v2/klarifikasi', callback=self.parse,\n",
        "                       headers={'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36'})\n",
        "\n",
        "    \n",
        "\n",
        "    def parse(self, response):\n",
        "        global article_date\n",
        "        article_container = response.xpath('//div[@id=\"hasil\"]/div[contains(@class,\"col-md-12\")]')\n",
        "\n",
        "        for article in article_container:\n",
        "            article_title=article.xpath('.//a/div/div/h3/text()').get()\n",
        "            article_date=article.xpath('.//a/div/div/div/text()').get()\n",
        "            link_author=article.xpath('.//a/@href').get()\n",
        "           \n",
        "            yield response.follow(url=link_author, callback=self.parse_author, meta={'article_title':article_title})\n",
        "\n",
        "        \n",
        "        next_page_url=response.xpath('//ul[@class=\"pagination justify-content-center\"]/li[8]/a/@href').get()\n",
        "\n",
        "        if next_page_url:\n",
        "                yield response.follow(url=next_page_url, callback=self.parse,\n",
        "                headers={'User-Agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.0.0 Safari/537.36'})\n",
        "\n",
        "\n",
        "      \n",
        "    def parse_author(self, response):\n",
        "        article_title=response.request.meta['article_title']\n",
        "        article_author=response.xpath('//div[@id=\"tabel_Keterangan\"]/div[contains(@class,\"row\")]/div[17]/text()').get()\n",
        "\n",
        "        yield{\n",
        "                'title':article_title,\n",
        "                'date':article_date,\n",
        "                'author':article_author\n",
        "            }"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jefe-6FsPFTd",
        "outputId": "57e9557b-f76a-4161-88fe-a4d05384ff84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing saberhoaks.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Crawl Data from turnbackhoax\n",
        "!scrapy crawl turnbackhoax -o turnbackhoax_all.csv"
      ],
      "metadata": {
        "id": "Jdw6h3PrQHxd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crawl Data from saberhoaks\n",
        "!scrapy crawl saberhoaks -o saberhoaks_all.csv"
      ],
      "metadata": {
        "id": "GEdEFVaQQQyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "File Hasil Scrapy ada di webscrapy/webscrapy/spiders dan download dulu untuk digunakan sebagai similarity check"
      ],
      "metadata": {
        "id": "PdAmbtxxQvtV"
      }
    }
  ]
}